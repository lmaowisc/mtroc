---
title: "Multiple imputations by two-level model"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Two-level-mi}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
bibliography: references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Set-up and notation

Suppose there are $m$ studies to evaluate the diagnostic accuracy of
some test. For the $i$th study, let

-   $\mbox{TP}_i$: number of true positives
-   $\mbox{FN}_i$: number of false negatives
-   $\mbox{FP}_i$: number of false positives
-   $\mbox{TN}_i$: number of true negatives

These are the data we wish to collect for each $i=1,\ldots, m$. Write
$N_i=\mbox{TP}_i+\mbox{FN}_i$, the number of cases;
$\bar N_i=\mbox{FP}_i + \mbox{TN}_i$, the number of non-cases; and
$n_i = N_i + \bar N_i$, the total sample size.

Let $p_i$ denote the sensitivity, $q_i$ the specificity, and $\pi_i$ the
prevalence of cases in the $i$th study. If there is no missing data, we
can easily estimate them by $\hat p_i=\mbox{TP}_i/N_i$,
$\hat q_i=\mbox{TN}_i/\bar N_i$, and $\hat\pi_i=N_i/n_i$, respectively.
We can also feed the entire data
$(\mbox{TP}_i, \mbox{FN}_i, \mbox{FP}_i, \mbox{TN}_i)$ $(i=1,\ldots, m)$
into any standard software, e.g., `mada` [@mada], for meta-analysis.

## Methods

### A motivating example

Consider the scenario where there are no non-cases in some study $i$, so
$\mbox{FN}_i$, $\mbox{TN}_i=$`NA`. An example:

```{r}
library(mtroc)
uts

```

We aim to fill in the missing values of $\mbox{FN}_i$ and $\mbox{TN}_i$
by **multiple imputations (MI)**.

### Two-level random-effects models

We can think of the data as generated in two stages. First, with total
sample size $n_i$, the relative size of $N_i$ vs $\bar N_i$ is
determined by prevalence $\pi_i$. Second, given $N_i$ and $\bar N_i$,
the relative size of $\mbox{TP}_i$ vs $\mbox{FN}_i$, and that of
$\mbox{FP}_i$ vs $\mbox{TN}_i$, are determined by sensitivity $p_i$ and
specificity $q_i$, respectively. This leads us to posit a two-level
model for prevalence and for sensitivity-specificity relationship: the
first allows us to impute the size of cases or non-cases; the second to
impute specificity from sensitivity (or vice versa).

To account for between-study heterogeneity, we add study-specific random
effects on the logit scale of each model. Write
${\rm logit}(x)=\log\{x/(1-x)\}$. 
\begin{align}\tag{1}
{\rm logit}(\pi_i)&= \alpha_0 + \varepsilon_i\\
{\rm logit}(q_i) &= (\beta_0 + b_{0i}) + (\beta_1 + b_{1i}){\rm logit}(p_i),
\end{align} 
where $(\alpha_0, \beta_0, \beta_1)$ are fixed effects,
$\varepsilon_i\sim N(0, \sigma_\pi^2)$,
$b_i=(b_{0i}, b_{1i})^{\rm T}\sim N(0, \Sigma_b)$, and
$\varepsilon_i\perp b_i$. In particular, model (1) maximally allows for
study-to-study variations in the prevalence and in the (generally
negative) correlation between sensitivity and specificity.

Let $\mathcal C$ denote the set of indices where complete data are
observed. Thus for each $i\in\mathcal C_i$, we have complete estimates
of $\hat\pi$, $\hat p_i$, and $\hat q_i$. Using them as
pseudo-observations in (1), we can estimate the parameters by
$\hat\alpha=\mbox{mean}\{{\rm logit}(\hat\pi_i): i\in\mathcal C\}$,
$\hat\sigma_\pi^2=\mbox{var}\{{\rm logit}(\hat\pi_i): i\in\mathcal C\}$,
$\hat\beta=$ least-squares estimate of
${\rm logit}(\hat q_i) \mbox{ vs }{\rm logit}(\hat p_i)$
$(i\in\mathcal C)$. Finally, $\Sigma_b$ can be estimated by
$\hat\Sigma_b=|\mathcal C|\hat{\mbox{var}}(\hat\beta)$, where
$|\mathcal C|$ is the number of studies in $\mathcal C$ and
$\hat{\mbox{var}}(\hat\beta)$ is the estimated variance matrix of
$\hat\beta$ under standard least squares.

### Imputation process

For any
$i\in\overline{\mathcal C}=\{i=1,\ldots, m: \mbox{FN}_i \mbox{ and } \mbox{TN}_i \mbox{ are missing}\}$,
we only know $N_i$ and $\hat p_i$, and will have to impute $\mbox{FN}_i$
and $\mbox{TN}_i$. We do so multiple times by drawing random effects
from model (1), in order to account for the certainty in the missing
values.

At the $j$th imputation, for each $i\in\overline{\mathcal C}$:

-   Draw $\varepsilon_i^{(j)}\sim N(0, \hat\sigma_\pi^2)$;
    $b_i^{(j)}\sim N(0,\hat\Sigma_b)$;
-   Compute $\hat\pi_i^{(j)}$ and $\hat q_i^{(j)}$ by inserting
    $\hat\alpha_0$, $\varepsilon_i^{(j)}$, $\hat\beta$, and $b_i^{(j)}$
    in model (1);
-   Impute the missing values by
    $\bar N_i^{(j)}= N_i(1-\hat\pi_i^{(j)})/\hat\pi_i^{(j)}$,
    $\mbox{FN}_i^{(j)} = \bar N_i^{(j)}(1-\hat q_i^{(j)})$, and
    $\mbox{TN}_i^{(j)} = \bar N_i^{(j)}\hat q_i^{(j)}$.

We thus obtain a completed dataset $$
\mathcal D^{(j)}=
\{(\mbox{TP}_i, \mbox{FN}_i, \mbox{FP}_i^{(j)}, \mbox{TN}_i^{(j)}): i=1,\ldots, m\},
$$ where we define $\mbox{FP}_i^{(j)}=\mbox{FP}_i$ and
$\mbox{TN}_i^{(j)}=\mbox{TN}_i$ for $i\in\mathcal C$. Given
$\mathcal D^{(j)}$, we can use standard software for meta-analysis to
obtain a summary measure of diagnostic accuracy, say, the summary AUC,
denoted by $\hat{\rm AUC}^{(j)}$. We may also obtain its variance
$\hat{\rm var}(\hat{\rm AUC}^{(j)})$.

Repeat the above for $j =1,\ldots, J$ (e.g., $J=500$). Then compute the
multiply imputed AUC by $$
\hat{\rm AUC}=J^{-1}\sum_{j=1}^J\hat{\rm AUC}^{(j)},
$$ and its variance by $$
\hat{\rm var}(\hat{\rm AUC}) = \underbrace{J^{-1}\sum_{j=1}^J(\hat{\rm AUC}^{(j)} -\hat{\rm AUC})^2}_{\rm between-imputation}
+ \underbrace{J^{-1}\sum_{j=1}^J\hat{\rm var}(\hat{\rm AUC}^{(j)})}_{\rm within-imputation}.
$$ The above computations can be done on the logit scale of the AUC to
improve their accuracy of approximation.

## References
